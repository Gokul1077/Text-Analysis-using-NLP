# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XJKGuq-f34NjBrChWOO70ntuzYeZ0K9P
"""

import pandas as pd
from bs4 import BeautifulSoup
import requests
import re

!pip install openpyxl

import csv

df = pd.read_excel('/content/drive/MyDrive/Sentiment_Analysis/Input.xlsx')

df.head()

df.shape

df.iloc[0,1]

link =df.iloc[0,1]
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',
}

source = requests.get(link, headers=headers).text
data = BeautifulSoup(source, 'lxml')
article = data.find('article')
title = article.h1.text
body =  article.find('div', class_='td-post-content').text
date = article.find('div', class_='td-module-meta-info').time.text

title

body

footer = article.find('div', class_='td-post-content').pre.text 
footer

body = body.replace(footer, '')

body

new_body = body.replace('\n',' ')
new_body = new_body.replace('\xa0',' ')

new_body

"""**Saving the Extracted text as URL_ID text file in the drive**"""

f = open("/content/drive/MyDrive/Sentiment_Analysis/Extracted_Folders/37.txt", "a")
f.write(new_body)
f.close()

"""**Main Source Input file**"""

f = open("/content/drive/MyDrive/Sentiment_Analysis/Extracted_Folders/37.txt", "r")
print(f.read())

"""**Reading and printing the Auditor StopWords**"""

with open('/content/drive/MyDrive/Sentiment_Analysis/Stop_Words/StopWords_Auditor.txt', 'r') as f:
    StopWords_Auditor = f.read().strip().split("\n")
StopWords_Auditor[-10:]

"""**Reading and printing the Dates and Numbers StopWords**"""

with open('/content/drive/MyDrive/Sentiment_Analysis/Stop_Words/StopWords_DatesandNumbers.txt', 'r') as f:
    StopWords_DatesandNumbers = f.read().strip().split("\n")
StopWords_DatesandNumbers[-10:]

"""**Reading and printing the Generic StopWords**"""

with open('/content/drive/MyDrive/Sentiment_Analysis/Stop_Words/StopWords_Generic.txt', 'r') as f:
    StopWords_Generic = f.read().strip().split("\n")
StopWords_Generic[-10:]

"""**Reading and printing the GenericLong StopWords**"""

with open('/content/drive/MyDrive/Sentiment_Analysis/Stop_Words/StopWords_GenericLong.txt', 'r') as f:
    StopWords_GenericLong= f.read().strip().split("\n")
StopWords_GenericLong[-10:]

"""**Reading and printing the Geographic StopWords**"""

with open('/content/drive/MyDrive/Sentiment_Analysis/Stop_Words/StopWords_Geographic.txt', 'r') as f:
    StopWords_Geographic= f.read().strip().split("\n")
StopWords_Geographic[-10:]

"""**Reading and printing the Names StopWords**"""

with open('/content/drive/MyDrive/Sentiment_Analysis/Stop_Words/StopWords_Names.txt', 'r') as f:
    StopWords_Names= f.read().strip().split("\n")
StopWords_Names[-10:]

"""**Reading and printing the Currencies StopWords**"""

with open('/content/drive/MyDrive/Sentiment_Analysis/Stop_Words/StopWords_Currencies.txt', 'rt') as f:
  StopWords_Currencies = [x.strip()[0:] for x in f.readlines()]
print(StopWords_Currencies)

"""**"Importing the Natural Language Processing Library"**"""

import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Open the input file
with open('/content/drive/MyDrive/Sentiment_Analysis/Extracted_Folders/37.txt', 'r') as f:
    text = f.read()
nltk.download('all')
# Tokenize the text
tokens = nltk.word_tokenize(text)

print(tokens)

"""**Number of Tokens(before removing stop_words)**"""

print("Number of Tokens(before removing stop_words):",len(tokens))

"""**1.1 Cleaning using Stop Words Lists**

**Removing the Auditor stop words**
"""

filtered_tokens = [token for token in tokens if (token.upper() or token.lower() or token.capitalize()) not in StopWords_Auditor]
# Print the filtered tokens
print(filtered_tokens)

print("Number of Tokens After removing Auditor stop_words :",len(filtered_tokens))

"""**Removing the Dates and Number Stop words**"""

filtered_tokens_DN = [token for token in filtered_tokens if (token.upper() or token.lower() or token.capitalize()) not in StopWords_DatesandNumbers]
# Print the filtered tokens
print(filtered_tokens_DN)

print("Number of Tokens After removing DatesandNumbers stop_words :",len(filtered_tokens_DN))

"""**Removing the Generic Stop words**"""

filtered_tokens_G = [token for token in filtered_tokens_DN if (token.upper() or token.lower() or token.capitalize()) not in StopWords_Generic]
# Print the filtered tokens
print(filtered_tokens_G)

print("Number of Tokens After removing Generic stop_words :",len(filtered_tokens_G))

"""**Removing the GenericLong Stop words**"""

filtered_tokens_GL = [token for token in filtered_tokens_G if (token.upper() or token.lower() or token.capitalize()) not in StopWords_GenericLong]
# Print the filtered tokens
print(filtered_tokens_GL)

print("Number of Tokens After removing Generic_Long stop_words :",len(filtered_tokens_GL))

"""**Removing the Geographic Stop words**"""

filtered_tokens_Geo = [token for token in filtered_tokens_GL if (token.upper() or token.lower() or token.capitalize()) not in StopWords_Geographic]
# Print the filtered tokens
print(filtered_tokens_Geo)

print("Number of Tokens After removing Geographic stop_words :",len(filtered_tokens_Geo))

"""**Removing the Names Stop words**"""

filtered_tokens_name = [token for token in filtered_tokens_Geo if (token.upper() or token.lower() or token.capitalize()) not in StopWords_Names]
# Print the filtered tokens
print(filtered_tokens_name)

print("Number of Tokens After removing Names stop_words :",len(filtered_tokens_name))

"""**Removing the Currency Stop words**"""

filtered_tokens_Cur = [token for token in filtered_tokens_name if (token.upper() or token.lower() or token.capitalize()) not in StopWords_Currencies]
# Print the filtered tokens
print(filtered_tokens_Cur)

print("Number of Tokens After removing Currency stop_words :",len(filtered_tokens_Cur))

"""**Calculating Total number of words after Cleaning:**

Eventhough we Filtered text from Stopwords but it contains unwanted characters like comma, quotation etc.
"""

l=filtered_tokens_Cur
print("Filtered from Stopwords but it contains unwanted characters like comma, quotation etc.")
print(l)

"""**Removing unwanted characters from Filtered List**"""

Filtered_List=[]
for i in l:
  if(i.isalnum()):
    Filtered_List.append(i)


print(Filtered_List)

Total_filterd_words=len(Filtered_List)
print("Total number of words after Cleaning:",Total_filterd_words)

"""**Converting the List to String : for calculating the Average Sentence Length (Later use)**"""

Org_Txt = ' '.join(map(str, l))
print(Org_Txt)

"""**Combining all Stop words in single list**"""

All_StopWords=[]
All_StopWords.extend(StopWords_Auditor)
All_StopWords.extend(StopWords_DatesandNumbers)
All_StopWords.extend(StopWords_Generic)
All_StopWords.extend(StopWords_GenericLong)
All_StopWords.extend(StopWords_Geographic)
All_StopWords.extend(StopWords_Names)
All_StopWords.extend(StopWords_Currencies)


print(All_StopWords,"\n",len(All_StopWords))

"""**Reading the Positive Words text file from Master dictionary:**"""

with open("/content/drive/MyDrive/Sentiment_Analysis/Master_Dictionary/positive-words.txt", "r") as f:
  positive_words = f.read().strip().split("\n")
print(positive_words)

print("Number of Positive words: ",len(positive_words))

"""**Removing stopwords present in the Positive words text file**"""

New_positive_words=[i for i in positive_words if i.upper() not in All_StopWords]
print(New_positive_words,"\n",len(New_positive_words))

"""**1.2 Creating a dictionary of Positive words**"""

posDict={}
for pw in New_positive_words:
  posDict[pw]=1

print(posDict)

"""**Reading the Negative Words text file from Master dictionary:**"""

with open("/content/drive/MyDrive/Sentiment_Analysis/Master_Dictionary/negative-words.txt", "r") as f:
  negative_words = f.read().strip().split("\n")
print(negative_words)

print("Number of Negative words: ",len(negative_words))

"""**Removing stopwords present in the Negative words text file**"""

New_negative_words=[i for i in negative_words if i.upper() not in All_StopWords]
print(New_negative_words,"\n",len(New_negative_words))

"""**1.2 Creating a dictionary of Negative words**"""

negDict={}
for pw in New_negative_words:
  negDict[pw]=-1

print(negDict)

"""**1.3 Extracting Derived variables**



> Positive Score


> Negative Score


> Polarity Score


> Subjective Score

**Calculating the Positive Score:**
"""

def Cal_PostiveScore(positive_score):
  for j in New_positive_words:
    for k in Filtered_List:
      if(j==k):
        positive_score=positive_score+1
  return positive_score

Positive_Score=Cal_PostiveScore(positive_score=0)
print("Positive Score: ",Positive_Score)

"""**Calculating the Negative Score:**"""

def Cal_NegativeScore(negative_score):
  for j in New_negative_words:
    for k in Filtered_List:
      if(j==k):
        negative_score=negative_score+1
  return negative_score

Negative_Score=Cal_NegativeScore(negative_score=0)
print("Negative Score: ",Negative_Score)

"""**Calculating the Polarity Score:**
**Range is from -1 to +1**
"""

Polarity_Score=(Positive_Score-Negative_Score)/(Positive_Score+Negative_Score+0.000001)
print(Polarity_Score)

"""**Calculating the Subjective Score:**"""

Subjective_Score=(Positive_Score+Negative_Score)/(Total_filterd_words+0.000001)
print(Subjective_Score)

"""**Importing Sentence Tokenizer from NLP Module**"""

from nltk.tokenize import sent_tokenize

"""**Number of Sentence in the web-scrapped Text**"""

no_sent=nltk.sent_tokenize(Org_Txt)

print(no_sent)
print("Number of Sentence : ",len(no_sent))

"""**5. Word Count**

**Number of Words in the Web-scrapped Text**
"""

from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
no_words = tokenizer.tokenize(Org_Txt)
print(no_words)
Word_Count=len(no_words)
print("Number of Words : ",len(no_words))

"""**3.Average Number of Words Per Sentence**"""

Avg_word_per_sent = len(no_words)/len(no_sent)
print("Average Sentence Length ", Avg_word_per_sent)

"""**6. Syllable Count Per Word**"""

vowels="aeiou"
Syllable_words=set()
Syllable_count=0
for word in no_words:
  if not(word.endswith('es')) and not(word.endswith('ed')):
    for i in range(len(word)):
      if(word[i] in vowels):
        Syllable_words.add(word)
        Syllable_count+=1

print("Syllable count : ",Syllable_count)
print("Syllable Words : ",Syllable_words)

print("Syllable count per word: ",Syllable_count/len(no_words))

"""**7. Personal Pronouns**"""

import re


# Use the \b boundary matcher to match individual words
pattern = r"\b\w+\b"

# Find all the words in the text
words = re.findall(pattern, Org_Txt)

# Count the number of occurrences of each word
occurrences = {}
for word in words:
    if word in occurrences:
        occurrences[word] += 1
    else:
        occurrences[word] = 1

print(occurrences)

Personnel_Pronoun=['i','we','my','ours','us','WE','I','MY','OURS','We','My','Ours','Us']
PPC=0
for key,value in occurrences.items():
  if(key in Personnel_Pronoun):
    print(key)
    PPC=PPC+value
print("Occurences of Peronnel Pronoun: ",PPC)

"""**8.Average Word Length**

**Sum of the total number of characters in each word**
"""

Total_char=0
for i in no_words:
  Total_char+=len(i)
print(Total_char)

Avg_word_len=Total_char/len(no_words)
print("Average Word Length",Avg_word_len)

"""**4. Complex Word Count**"""

Complex_Word=[]
Syllable_List=list(Syllable_words)
for word in Syllable_List:
  count=0
  for j in range(len(word)):
    if(count>2):
      Complex_Word.append(word)
      break
    if(word[j] in vowels):
      count=count+1

print("Complex word ",Complex_Word)
Comp_word_count=len(Complex_Word)
print("Complex word Count: ",len(Complex_Word))

"""**2. Analysis of Readability**

**Calculating Average Sentence Length**
"""

Avg_sent_len=len(no_words)/len(no_sent)
print("Average Sentence Length ",Avg_sent_len)

"""**Percentage of Complex words**"""

no_comp=len(Complex_Word)
percent_comp=(no_comp/len(no_words))*100
print("Percentage of Complex words :",percent_comp)

"""**Fog Index**"""

Fog_index=0.4 * (Avg_sent_len + percent_comp)
print("Fog index : ",Fog_index)

"""**Saving the Output data in Excel format**"""

import openpyxl

workbook = openpyxl.Workbook()

# Get the active worksheet in the workbook
worksheet = workbook.active

# Write data to the worksheet
worksheet.append(['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE',
                     'COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])
workbook.save('/content/drive/MyDrive/Sentiment_Analysis/Output.xlsx')

"""**Saving the Output Variables in the Excel Sheet**"""

worksheet.append([df.iloc[0,0],df.iloc[0,1],Positive_Score,Negative_Score,Polarity_Score,Subjective_Score,Avg_sent_len,percent_comp,Fog_index,Avg_word_per_sent,Comp_word_count,
                  Word_Count,Syllable_count,PPC,Avg_word_len])
workbook.save('/content/drive/MyDrive/Sentiment_Analysis/Output.xlsx')